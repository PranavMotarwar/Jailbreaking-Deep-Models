# Jailbreaking Deep Models: Adversarial Attacks  
![License](https://img.shields.io/badge/License-MIT-blue)

**Authors**: Aniket Mane, Subhan Akhtar, Pranav Motarwar  
**Affiliation**: New York University  
**GitHub**: [Repository Link](https://github.com/PranavMotarwar/Jailbreaking-Deep-Models)  

---

## üìÇ Repository Contents
| File/Folder       | Description                                  |
|--------------------|----------------------------------------------|
| `DL_project_3.ipynb` | Jupyter Notebook with full implementation    |
| `task5_transferability.png` | Graph for Task 5 results                  |
| `Deep_Learning_Mini_Project_3.pdf` | Project report (PDF)             |
| `TestDataSet/`     | Test images (unzip `TestDataSet.zip`)        |
| `AdversarialSets/` | Generated FGSM/PGD/Patch adversarial images  |

---

## ‚ñ∂Ô∏è Quick Start
1. **Run in Colab**:  
   [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yourusername/jailbreaking-deep-models/blob/main/DL_project_3.ipynb)

2. **Local Execution**:
   ```bash
   # Install dependencies
   pip install -r requirements.txt
   
   # Launch Jupyter notebook
   jupyter notebook DL_project_3.ipynb
